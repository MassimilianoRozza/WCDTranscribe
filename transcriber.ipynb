{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "This Python script for Google Colab automates the batch transcription of Italian audio files using OpenAI's Whisper model. Audio files are processed directly from Google Drive, and the resulting transcriptions are saved as text files in the same folder.\n",
        "\n",
        "## Main Features\n",
        "\n",
        "- Batch Transcription: Automatically processes multiple audio files in sequence.\n",
        "- Organized File Management: Relies on a specific file naming structure to organize transcriptions.\n",
        "- Flexible Lesson Identifiers: Allows the use of both numbers and dates (or any string without dots \".\") as lesson identifiers.\n",
        "- Google Drive Integration: Directly accesses audio files and saves transcriptions to Google Drive.\n",
        "- Resumption from Interruptions: Able to resume transcriptions even if the process is interrupted.\n",
        "\n",
        "## Variable Description\n",
        "\n",
        "- `model_name`: Specifies the Whisper model to use (\"tiny\", \"base\", \"small\", \"medium\", \"large\"). Larger models offer greater accuracy but require more time and resources.\n",
        "- `lang`: Sets the audio file language to \"it\" (Italian).\n",
        "- `in_folder`: Path on Google Drive where the audio files to be transcribed are located.\n",
        "- `out_folder`: Path on Google Drive where the transcriptions will be saved.\n",
        "\n",
        "## Audio File Structure\n",
        "\n",
        "Audio files must be organized in the `in_folder` folder using the following naming convention:\n",
        "\n",
        "`<lesson_identifier>.<recording_number>.<extension>`\n",
        "\n",
        "Where `<lesson_identifier>` can be a number or a date (or any string without dots \".\").\n",
        "\n",
        "### Examples:\n",
        "\n",
        "- `1.1.m4a` (first recording of lesson 1)\n",
        "- `2023-10-27.1.mp3` (first recording of the lesson from October 27, 2023)\n",
        "- `AdvancedCourse.2.wav` (second recording of the AdvancedCourse lesson)\n",
        "\n",
        "## Transcription Structure\n",
        "\n",
        "Transcriptions are saved in the `out_folder` folder using the following naming convention:\n",
        "\n",
        "`<lesson_identifier>.txt`\n",
        "\n",
        "### Examples:\n",
        "\n",
        "- `1.txt` (contains the transcription of 1.1.m4a)\n",
        "- `2023-10-27.txt` (contains the transcription of 2023-10-27.1.mp3)\n",
        "- `AdvancedCourse.txt` (contains the transcription of AdvancedCourse.2.wav)\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "- Dependency Installation:\n",
        "  - Installs the `openai-whisper` and `ffmpeg` libraries.\n",
        "- Google Drive Mounting:\n",
        "    - Mounts Google Drive to access audio files and save transcriptions.\n",
        "- Whisper Model Loading:\n",
        "    - Loads the specified Whisper model.\n",
        "- Existing Transcription Management:\n",
        "    - Scans the output folder to identify already transcribed lessons, avoiding duplicates.\n",
        "- Audio File Processing:\n",
        "    - Iterates through the audio files in the input folder.\n",
        "    - Checks if the lesson has already been transcribed.\n",
        "    - Transcribes the audio files using the Whisper model.\n",
        "    - Handles any errors during transcription.\n",
        "    - Saves transcriptions to the corresponding text files.\n",
        "- Status Output:\n",
        "    - Prints informative messages during processing to monitor the status\n",
        "\n",
        "# Descrizione\n",
        "Questo script Python per Google Colab automatizza la trascrizione batch di file audio in italiano utilizzando il modello Whisper di OpenAI. I file audio vengono elaborati direttamente da Google Drive, e le trascrizioni risultanti sono salvate come file di testo nella stessa cartella.\n",
        "\n",
        "## Caratteristiche Principali\n",
        "\n",
        "- Trascrizione Batch: Elabora automaticamente più file audio in sequenza.\n",
        "- Gestione di File Organizzati: Si basa su una struttura di nomi di file specifica per organizzare le trascrizioni.\n",
        "- Identificatori di Lezione Flessibili: Permette di utilizzare sia numeri che date (o qualsiasi altra stringa senza punti \".\") come identificatori di lezione.\n",
        "- Integrazione con Google Drive: Accede direttamente ai file audio e salva le trascrizioni su Google Drive.\n",
        "- Ripresa da Interruzioni: è in grado di riprendere le trascrizioni anche se il processo viene interrotto.\n",
        "\n",
        "## Descrizione delle Variabili\n",
        "\n",
        "- `model_name`: Specifica il modello Whisper da utilizzare (\"tiny\", \"base\", \"small\", \"medium\", \"large\"). Modelli più grandi offrono maggiore accuratezza, ma richiedono più tempo e risorse.\n",
        "- `lang`: Imposta la lingua dei file audio su \"it\" (italiano).\n",
        "- `in_folder`: Percorso su Google Drive dove si trovano i file audio da trascrivere.\n",
        "- `out_folder`: Percorso su Google Drive dove verranno salvate le trascrizioni.\n",
        "\n",
        "## Struttura dei File Audio\n",
        "\n",
        "I file audio devono essere organizzati nella cartella in_folder con la seguente convenzione di denominazione:\n",
        "\n",
        "<identificatore_lezione>.<numero_registrazione>.<estensione>\n",
        "\n",
        "Dove <identificatore_lezione> può essere un numero o una data (o qualsiasi stringa senza punti \".\").\n",
        "\n",
        "### Esempi:\n",
        "\n",
        "- `1.1.m4a` (prima registrazione della lezione 1)\n",
        "- '2023-10-27.1.mp3` (prima registrazione della lezione del 27 ottobre 2023)\n",
        "- `CorsoAvanzato.2.wav` (seconda registrazione della lezione CorsoAvanzato)\n",
        "\n",
        "## Struttura delle Trascrizioni:\n",
        "\n",
        "Le trascrizioni vengono salvate nella cartella out_folder con la seguente convenzione di denominazione:\n",
        "\n",
        "<identificatore_lezione>.txt\n",
        "\n",
        "### Esempi:\n",
        "\n",
        "- `1.txt` (contiene la trascrizione di 1.1.m4a)\n",
        "- `2023-10-27.txt` (contiene la trascrizione di 2023-10-27.1.mp3)\n",
        "- `CorsoAvanzato.txt` (contiene la trascrizione di CorsoAvanzato.2.wav)\n",
        "\n",
        "## Dettagli Implementativi\n",
        "- Installazione delle Dipendenze:\n",
        "  - Installa le librerie openai-whisper e ffmpeg.\n",
        "- Montaggio di Google Drive:\n",
        "    - Monta Google Drive per accedere ai file audio e salvare le trascrizioni.\n",
        "- Caricamento del Modello Whisper:\n",
        "    - Carica il modello Whisper specificato.\n",
        "- Gestione delle Trascrizioni Esistenti:\n",
        "    - Scansiona la cartella di output per identificare le lezioni già trascritte, in modo da evitare duplicati.\n",
        "- Elaborazione dei File Audio:\n",
        "    - Itera attraverso i file audio nella cartella di input.\n",
        "    - Controlla se la lezione è già stata trascritta.\n",
        "    - Trascrive i file audio utilizzando il modello Whisper.\n",
        "    - Gestisce eventuali errori durante la trascrizione.\n",
        "    - Salva le trascrizioni nei file di testo corrispondenti.\n",
        "- Output di Stato:\n",
        "    - Stampa messaggi informativi durante l'elaborazione per monitorare lo stato."
      ],
      "metadata": {
        "id": "G9FBRmxSVVRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lOT5Txbpiuq_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U openai-whisper;\n",
        "!sudo apt install ffmpeg;\n",
        "import whisper\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_name = \"large\" #whisper model name\n",
        "lang = \"it\" # audio language\n",
        "in_folder = \"drive/MyDrive/audio-files/\" #audio files folder\n",
        "out_folder = \"drive/MyDrive/audio-files/\" #transcriptions folder"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h0iUXtKri6WD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "done = [] #files already transcripted\n",
        "model = whisper.load_model(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti5B4RitW9DY",
        "outputId": "6422b392-ddff-40f4-ff1a-df7b15720301"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(out_folder, exist_ok=True)  # Crea la cartella se non esiste\n",
        "transcriptions = os.listdir(out_folder)\n",
        "for file in transcriptions:\n",
        "    if file.endswith(\".txt\"):\n",
        "      done.append(file.split(\".\")[0])\n",
        "print(done)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMXJ00HBNZPF",
        "outputId": "a779bf9a-d4de-4c98-829d-c206896e3d88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audios = os.listdir(in_folder)\n",
        "for file in audios:\n",
        "  print(\"ELABORATION: \" + file)\n",
        "  if file.split(\".\")[0] in done:\n",
        "    print(\"SKIP: \" + file)\n",
        "    continue\n",
        "  else:\n",
        "    print(\"TRANSCRIBING: \" + file)\n",
        "    try:\n",
        "      result = model.transcribe(in_folder + file, language=lang)\n",
        "    except whisper.WhisperError as e:\n",
        "      print(f\"ERROR while working on {file}: {e} \")\n",
        "    print(\"SAVING: \" + file + \" transcription\")\n",
        "    with open(out_folder + file.split(\".\")[0] + \".txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "      f.write(result[\"text\"])\n",
        "    print(\"DONE: \" + file)\n",
        "    print(\"=======================================\")\n",
        "\n",
        "print(\"======== END ========\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ifNlGeOiYS",
        "outputId": "5f9aeddb-398e-4a4c-c11a-7cd6e2d0ffbc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELABORATION: 1.1.m4a\n",
            "SKIP: 1.1.m4a\n",
            "ELABORATION: 1.2.m4a\n",
            "SKIP: 1.2.m4a\n",
            "ELABORATION: 1.3.m4a\n",
            "SKIP: 1.3.m4a\n",
            "ELABORATION: 2.1.m4a\n",
            "SKIP: 2.1.m4a\n",
            "ELABORATION: 2.2.m4a\n",
            "SKIP: 2.2.m4a\n",
            "ELABORATION: 2.3.m4a\n",
            "SKIP: 2.3.m4a\n",
            "ELABORATION: 1.txt\n",
            "SKIP: 1.txt\n",
            "ELABORATION: 2.txt\n",
            "SKIP: 2.txt\n",
            "======== END ========\n"
          ]
        }
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "GjYcLi76Z91G"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}